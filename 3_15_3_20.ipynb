{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.15-3.20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9At+oWYJiMs0VNjGaY8ZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kmatsu-tokudai/YSEC/blob/master/3_15_3_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10np3DMpzry",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "49b1ab2f-b5d8-4aa3-db85-41224b0351f2"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() # kaggle.jsonをアップロード\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-272f878e-9d70-452d-a18d-9bb471c869ab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-272f878e-9d70-452d-a18d-9bb471c869ab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMjmDZrAp_vM"
      },
      "source": [
        "# 3.15～3.20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWwqaVkyrCwn"
      },
      "source": [
        "### プログラム3.15 SelectKBestとSelectPercentileによる特徴選択\n",
        "#### フィルター法としてカイ二乗検定、ANOVAを使って特徴選択"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSa_lAypE_EA",
        "outputId": "321bd7cf-e887-4efb-8a5d-801257893ad6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import \\\n",
        "RandomForestClassifier as RandomForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 上位k個の特徴量を選択するクラス、\n",
        "# 上位percentileパーセントの特徴量を選択するクラス\n",
        "from sklearn.feature_selection import SelectKBest, \\\n",
        " SelectPercentile\n",
        "# カイ二乗検定、分散分析のモジュールをインポート\n",
        "from sklearn.feature_selection import chi2, f_classif\n",
        "# ピアソンの積率相関係数のモジュールをインポート\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ピアソンの積率相関係数\n",
        "def Pearson_corr_coeff(X, y):\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    #gg = f_classif(X, y)\n",
        "    #print(gg)\n",
        "    pr = [[], []]\n",
        "    for i in range(X.shape[1]):\n",
        "        x = X[:, i]\n",
        "        scores = pearsonr(y, x)\n",
        "        #scores = f_classif(y, x)\n",
        "        pr[0].append(scores[0])\n",
        "        pr[1].append(scores[1])\n",
        "\n",
        "    #pr = np.asarray(np)\n",
        "    print(pr)\n",
        "    return pr\n",
        "\n",
        "# データの準備\n",
        "def prepare():\n",
        "    !kaggle datasets download -d \\\n",
        "    uciml/mushroom-classification\n",
        "    !unzip mushroom-classification.zip\n",
        "    \n",
        "# カテゴリ変数を数字に変換\n",
        "def replace_to_digit( dff ):\n",
        "    t = list(set(dff.values.tolist()))\n",
        "    t.sort()\n",
        "    vals = [t.index(v)+1 for v in dff.values]\n",
        "    return vals\n",
        "\n",
        "# 前処理(欠損値の除去、カテゴリ変数の数値への変換)\n",
        "def preprocess():\n",
        "    df = pd.read_csv('mushrooms.csv')\n",
        "    df = df.replace('?', 'NaN')\n",
        "    df = df.dropna(how = 'any')\n",
        "    df['class'] = df['class'].map(\n",
        "                  {'p': 1, 'e': 0}).astype(int)\n",
        "    features = []\n",
        "    for c in df.columns.values:\n",
        "      if c != 'class':\n",
        "        features.append(c)\n",
        "        df[c] = replace_to_digit(df[c])\n",
        "    X_train = df.loc[:, features].values\n",
        "    y_train = df.loc[:, ['class']].values.ravel()\n",
        "    return X_train, y_train, features\n",
        "\n",
        "# 特徴選択(SelectKBestを用いる)\n",
        "def select_feature(selector_type, \n",
        "n_features, X_train, y_train, features):\n",
        "    selector = SelectKBest(\n",
        "     score_func=selector_type, \n",
        "     k=n_features)\n",
        "    # n_features次元の特徴量に変換\n",
        "    X_new = selector.fit_transform(X_train, y_train) \n",
        "    feature_scores = list(zip(selector.scores_, \n",
        "                          features))\n",
        "    # 特徴スコア順にソート\n",
        "    sorted_feature_scores = sorted(feature_scores,\n",
        "                                   reverse=True)\n",
        "    selected_f = []\n",
        "    # 特徴スコアの高い順に表示\n",
        "    for i, fs in enumerate(\n",
        "                     sorted_feature_scores[:n_features]):\n",
        "        if fs[0] > 0 and fs[0] != None:\n",
        "            print('[%d]: %s\\t%.2lf' % (i+1, fs[1], fs[0]) )\n",
        "            selected_f.append(fs[1])\n",
        "        s_f = list(map(lambda i: features[i], \n",
        "                filter(\n",
        "                lambda i: features[i] in selected_f, range( \\\n",
        "                              len(features)))))\n",
        "    return s_f\n",
        "\n",
        "# 特徴選択(SelectPercentile)\n",
        "def select_feature_percentile(selector_type, \n",
        "                              percentile, \n",
        "                              X_train, y_train, \n",
        "                              features):\n",
        "    selector = SelectPercentile(\n",
        "                           score_func=selector_type,\n",
        "                           percentile=percentile)\n",
        "    \n",
        "\n",
        "    # n_features次元の特徴量に変換\n",
        "    X_new = selector.fit_transform(X_train, y_train) \n",
        "    # 選択された特徴の数を取得\n",
        "    sel_count = np.count_nonzero(\n",
        "                     selector.get_support()==True) \n",
        "    feature_scores = list(\n",
        "                      zip(selector.scores_, \n",
        "                      features))\n",
        "    # 特徴スコア順にソート\n",
        "    sorted_feature_scores = sorted(\n",
        "                            feature_scores, \n",
        "                            reverse=True)\n",
        "    selected_f = []\n",
        "\n",
        "    # 特徴スコアの高い順に表示\n",
        "    for i, fs in enumerate( sorted_feature_scores[:sel_count]):\n",
        "        if fs[0] > 0 and fs[0] != None:\n",
        "            print('[%d]: %s\\t%.2lf' % (i+1, fs[1], fs[0]) )\n",
        "            selected_f.append(fs[1])\n",
        "    s_f = list(map(lambda i: features[i], \n",
        "                filter(lambda i: features[i] \\\n",
        "                   in selected_f, range(len(features)))))\n",
        "    return s_f\n",
        "\n",
        "def main():\n",
        "    #prepare()\n",
        "    X_train, y_train, features = preprocess()\n",
        "    target_names = ['edible', 'poisonous']\n",
        "\n",
        "    print('[Original Features]\\n%s' % '\\n'.join(features))\n",
        "    # 特徴選択手法\n",
        "    selectors = {'chi2': chi2,\n",
        "                  'ANOVA': f_classif,\n",
        "                  'Pearson': Pearson_corr_coeff}\n",
        "    # SelectKBestを使う場合(n_features個の特徴量を選択)\n",
        "    n_features = 5\n",
        "    X_tr, X_te, y_tr, y_te = \\\n",
        "         train_test_split(X_train, y_train, \n",
        "         train_size=0.7, random_state=0)\n",
        "    df_sel = pd.DataFrame(X_tr, columns=features)\n",
        "    df_sel_te = pd.DataFrame(X_te, columns=features)\n",
        "    for selN, selector_type in selectors.items():\n",
        "        print('\\n-*-*-*- Select by %s -*-*-*-' % selN)\n",
        "        s_f =select_feature(selector_type, \n",
        "            n_features, X_tr, y_tr, features)\n",
        "        X_tr_sel = df_sel.loc[:, s_f].values\n",
        "        X_te_sel = df_sel_te.loc[:, s_f].values\n",
        "        rf = RandomForest(n_estimators=100, \n",
        "          max_depth=4, random_state=0)\n",
        "        rf.fit(X_tr_sel, y_tr)\n",
        "        y_pred = rf.predict(X_te_sel)\n",
        "        print(classification_report(y_te, y_pred,\n",
        "                    target_names=target_names, \n",
        "                    zero_division=1))\n",
        "\n",
        "    # SelectPercentileを使う場合\n",
        "    percentile = 5\n",
        "    for selN, selector_type in selectors.items():\n",
        "        print('\\n-*-*-*- Select by %s -*-*-*-' % selN)\n",
        "        s_f =select_feature_percentile(selector_type, \n",
        "                        percentile, X_train, y_train, \n",
        "                        features )\n",
        "        X_tr_sel = df_sel.loc[:, s_f].values\n",
        "        X_te_sel = df_sel_te.loc[:, s_f].values\n",
        "        rf = RandomForest(n_estimators=100, \n",
        "                          max_depth=4, random_state=0)\n",
        "        rf.fit(X_tr, y_tr)\n",
        "        y_pred = rf.predict(X_te)\n",
        "        print(classification_report(y_te, y_pred,\n",
        "                    target_names=target_names, \n",
        "                    zero_division=1))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Original Features]\n",
            "cap-shape\n",
            "cap-surface\n",
            "cap-color\n",
            "bruises\n",
            "odor\n",
            "gill-attachment\n",
            "gill-spacing\n",
            "gill-size\n",
            "gill-color\n",
            "stalk-shape\n",
            "stalk-root\n",
            "stalk-surface-above-ring\n",
            "stalk-surface-below-ring\n",
            "stalk-color-above-ring\n",
            "stalk-color-below-ring\n",
            "veil-type\n",
            "veil-color\n",
            "ring-number\n",
            "ring-type\n",
            "spore-print-color\n",
            "population\n",
            "habitat\n",
            "\n",
            "-*-*-*- Select by chi2 -*-*-*-\n",
            "[1]: gill-color\t3504.00\n",
            "[2]: ring-type\t897.99\n",
            "[3]: stalk-root\t430.39\n",
            "[4]: habitat\t295.17\n",
            "[5]: gill-size\t276.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      edible       0.96      0.96      0.96      1272\n",
            "   poisonous       0.96      0.96      0.96      1166\n",
            "\n",
            "    accuracy                           0.96      2438\n",
            "   macro avg       0.96      0.96      0.96      2438\n",
            "weighted avg       0.96      0.96      0.96      2438\n",
            "\n",
            "\n",
            "-*-*-*- Select by ANOVA -*-*-*-\n",
            "[1]: gill-size\t2399.59\n",
            "[2]: gill-color\t2264.93\n",
            "[3]: bruises\t1871.94\n",
            "[4]: stalk-root\t930.28\n",
            "[5]: gill-spacing\t806.29\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      edible       0.94      0.99      0.96      1272\n",
            "   poisonous       0.98      0.93      0.96      1166\n",
            "\n",
            "    accuracy                           0.96      2438\n",
            "   macro avg       0.96      0.96      0.96      2438\n",
            "weighted avg       0.96      0.96      0.96      2438\n",
            "\n",
            "\n",
            "-*-*-*- Select by Pearson -*-*-*-\n",
            "[[0.05963674971463793, 0.18225803415991892, -0.03136311395566459, -0.4977386179380059, -0.09627158050698083, 0.11901921977348522, -0.3524629007101222, 0.5448370972182458, -0.5337928957722362, -0.10888697082414725, -0.3750289975830018, -0.32036293279952555, -0.2874189859274285, -0.155531566717614, -0.1440041496503759, nan, 0.14000438698716974, -0.21988809068472895, -0.3981054899423524, 0.17223657174102663, 0.2987113084978848, 0.20906436281928437], [6.79663461347828e-06, 1.1627313820862018e-43, 0.01802932670729727, 0.0, 3.4714147799532466e-13, 2.1613493545673512e-19, 5.615262833404612e-166, 0.0, 0.0, 1.8202091411833403e-16, 2.3232930885085006e-189, 7.246171622811125e-136, 1.3936277844999267e-108, 4.034058569894683e-32, 9.962731657647416e-28, nan, 2.762816330478967e-26, 3.2527439517652475e-63, 2.7901629554435586e-215, 4.139155087525609e-39, 1.5496930523509138e-117, 3.475704950029914e-57]]\n",
            "[1]: gill-size\t0.54\n",
            "[2]: cap-surface\t0.18\n",
            "[4]: population\t0.30\n",
            "[5]: habitat\t0.21\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      edible       0.86      0.90      0.88      1272\n",
            "   poisonous       0.89      0.84      0.86      1166\n",
            "\n",
            "    accuracy                           0.87      2438\n",
            "   macro avg       0.88      0.87      0.87      2438\n",
            "weighted avg       0.87      0.87      0.87      2438\n",
            "\n",
            "\n",
            "-*-*-*- Select by chi2 -*-*-*-\n",
            "[1]: gill-color\t4932.45\n",
            "[2]: ring-type\t1358.08\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      edible       0.98      1.00      0.99      1272\n",
            "   poisonous       1.00      0.98      0.99      1166\n",
            "\n",
            "    accuracy                           0.99      2438\n",
            "   macro avg       0.99      0.99      0.99      2438\n",
            "weighted avg       0.99      0.99      0.99      2438\n",
            "\n",
            "\n",
            "-*-*-*- Select by ANOVA -*-*-*-\n",
            "[1]: gill-size\t3343.70\n",
            "[2]: gill-color\t3182.11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      edible       0.98      1.00      0.99      1272\n",
            "   poisonous       1.00      0.98      0.99      1166\n",
            "\n",
            "    accuracy                           0.99      2438\n",
            "   macro avg       0.99      0.99      0.99      2438\n",
            "weighted avg       0.99      0.99      0.99      2438\n",
            "\n",
            "\n",
            "-*-*-*- Select by Pearson -*-*-*-\n",
            "[[0.05295056443436223, 0.17844612596168047, -0.03138408686213253, -0.5015303774075602, -0.09355164402403797, 0.12919986026788394, -0.34838678518427585, 0.5400243574330179, -0.5305661908665221, -0.10201901701656126, -0.3793609848063995, -0.33459274937291483, -0.2988005521949008, -0.15400272516021626, -0.14673028309667255, nan, 0.1451415915101194, -0.21436647197799347, -0.4117713859393494, 0.17196097382990108, 0.2986855359732813, 0.21717927764282297], [1.7950984233236668e-06, 4.1667175524803675e-59, 0.004669212113633461, 0.0, 2.9293064350260065e-17, 1.393189044005069e-31, 1.5638586260569165e-230, 0.0, 0.0, 3.028417773754335e-20, 2.0859629620818077e-276, 1.0517900442637717e-211, 3.3862271393551446e-167, 2.658955309646993e-44, 2.4662658778605718e-40, nan, 1.707320319531556e-39, 4.4432567984837986e-85, 0.0, 5.8959419349985485e-55, 4.602142583127273e-167, 2.492791647382976e-87]]\n",
            "[1]: gill-size\t0.54\n",
            "[2]: cap-surface\t0.18\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      edible       0.98      1.00      0.99      1272\n",
            "   poisonous       1.00      0.98      0.99      1166\n",
            "\n",
            "    accuracy                           0.99      2438\n",
            "   macro avg       0.99      0.99      0.99      2438\n",
            "weighted avg       0.99      0.99      0.99      2438\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afK_ydW3q7RQ"
      },
      "source": [
        "### プログラム3.16 相互情報量(MI)を使った特徴選択"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X517ywksrRhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43bc8c46-348a-4276-8cf7-a44f4f3ce091"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "# 相互情報量を計算するメソッドをインポート\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# データの準備\n",
        "def prepare():\n",
        "    !kaggle datasets download -d pmenshih/kpmi-mbti-mod-test \n",
        "    !unzip kpmi-mbti-mod-test.zip\n",
        "\n",
        "# 前処理（使用する項目の絞り込み、正規化）\n",
        "def preprocess():\n",
        "    # MBTI診断データを読込み\n",
        "    df = pd.read_csv('kpmi_data.csv', sep=';')\n",
        "    # 現在の職業に満足しているかどうか(yes:1, no:0)\n",
        "    y = df.loc[:, 'satisfied'].values\n",
        "    # 用いる特徴量（MBTIのスコア)\n",
        "    scales = ['scale_e','scale_i','scale_s',\n",
        "  'scale_n','scale_t','scale_f',\n",
        "  'scale_j','scale_p']\n",
        "    df = pd.DataFrame(df.loc[:,scales], columns=scales)\n",
        "    print(df)\n",
        "    X = df.loc[:, df.columns.values].values\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y,\n",
        "   random_state=0, train_size=0.9)\n",
        "    return x_train, x_test, y_train, y_test, scales\n",
        "\n",
        "# Random Forest で分類評価\n",
        "def predict_satisfaction(x_train, x_test, y_train, y_test):\n",
        "    clf = RandomForestClassifier(max_depth=4, random_state=2)\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    print('Accuracy = {:.3f}'.format(accuracy_score(\n",
        "  y_test, y_pred)))\n",
        "    labels = ['no', 'yes']\n",
        "    print(classification_report(y_test, y_pred,\n",
        "   target_names=labels))\n",
        "# MI (相互情報量)を使って特徴選択\n",
        "def select_feature_by_MI(x_train, x_test, \n",
        "        y_train, y_test, \n",
        "        scales, n_features): \n",
        "    # n_featuers 個の特徴を選択\n",
        "    selecter = SelectKBest(\n",
        "        mutual_info_classif, k=n_features).fit(\n",
        "                     x_train, y_train)\n",
        "    \n",
        "\n",
        "    sel_features = []\n",
        "    selected_feature = selecter.get_support()\n",
        "    for i in range(len(selected_feature)):\n",
        "      if selected_feature[i]:\n",
        "        print('Selected Feature - {}'.format(scales[i])) \n",
        "        sel_features.append(scales[i])\n",
        "    trdf = pd.DataFrame(x_train, columns=scales) \n",
        "    tedf = pd.DataFrame(x_test, columns=scales)\n",
        "    x_train = trdf.loc[:, sel_features].values\n",
        "    x_test = tedf.loc[:, sel_features].values\n",
        "    return x_train, x_test\n",
        "\n",
        "def main():\n",
        "    prepare()\n",
        "    x_train, x_test, y_train, y_test, scales = preprocess()\n",
        "    print('- 特徴選択無し [%d個の特徴量] -' % len(scales))\n",
        "    predict_satisfaction(x_train, x_test, y_train, y_test)\n",
        "    n_features = 3\n",
        "    print('- MIによる特徴選択 [%d個の特徴量] -' % n_features)\n",
        "    x_train, x_test = select_feature_by_MI(x_train, x_test,\n",
        "   y_train, y_test, scales, n_features)\n",
        "    predict_satisfaction(x_train, x_test, y_train, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading kpmi-mbti-mod-test.zip to /content\n",
            "\r  0% 0.00/985k [00:00<?, ?B/s]\n",
            "\r100% 985k/985k [00:00<00:00, 64.2MB/s]\n",
            "Archive:  kpmi-mbti-mod-test.zip\n",
            "  inflating: kpmi_data.csv           \n",
            "  inflating: kpmi_key.json           \n",
            "  inflating: questionnaire_schema.json  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "       scale_e  scale_i  scale_s  scale_n  scale_t  scale_f  scale_j  scale_p\n",
            "0            5       26       25       12       24        8       27       10\n",
            "1           16       22       17       16       23       11       22       15\n",
            "2           28        6       14       18       20       16       20       15\n",
            "3           28       10       22       10       16       14       22       12\n",
            "4           24       10       16       17       13       23       31        3\n",
            "...        ...      ...      ...      ...      ...      ...      ...      ...\n",
            "21841       25       15       17       20       13       13       19       15\n",
            "21842       26        6       20       18       11       21       22       11\n",
            "21843       16       21       22       11       28        5       25        9\n",
            "21844       21       13       12       17        8       30       30        5\n",
            "21845       22       16       22       15       29        6       29        6\n",
            "\n",
            "[21846 rows x 8 columns]\n",
            "- 特徴選択無し [8個の特徴量] -\n",
            "Accuracy = 0.695\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.55      0.12      0.19       682\n",
            "         yes       0.70      0.96      0.81      1503\n",
            "\n",
            "    accuracy                           0.69      2185\n",
            "   macro avg       0.63      0.54      0.50      2185\n",
            "weighted avg       0.66      0.69      0.62      2185\n",
            "\n",
            "- MIによる特徴選択 [3個の特徴量] -\n",
            "Selected Feature - scale_e\n",
            "Selected Feature - scale_i\n",
            "Selected Feature - scale_n\n",
            "Accuracy = 0.698\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.57      0.13      0.21       682\n",
            "         yes       0.71      0.96      0.81      1503\n",
            "\n",
            "    accuracy                           0.70      2185\n",
            "   macro avg       0.64      0.54      0.51      2185\n",
            "weighted avg       0.67      0.70      0.63      2185\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9AOrU6TrcMS"
      },
      "source": [
        "### プログラム3.17 SVM-RFEを用いた特徴選択"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbjI9cfhrjJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154d1680-a378-4b75-d5d7-623f70252922"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVC\n",
        "# 正規化のモジュールをインポート\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report as clf_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# データの準備\n",
        "def prepare():\n",
        "    !kaggle datasets download -d \\\n",
        "dileep070/heart-disease-prediction-using-logistic-regression\n",
        "    !unzip \\\n",
        "    heart-disease-prediction-using-logistic-regression.zip\n",
        "    \n",
        "    df = pd.read_csv('framingham.csv')\n",
        "    # 心臓病データセットのデータを使用\n",
        "    # 10年後の冠状動脈性心臓病CHDの\n",
        "    # 発症リスクの有無(0:無, 1:有)\n",
        "    features = []\n",
        "    # 分類に使用する特徴量（最後の列以外すべて）\n",
        "    for i, f in enumerate(df.columns.values ):\n",
        "      if i != len(df.columns)-1:\n",
        "        features.append(f)\n",
        "  \n",
        "    X_train = df.loc[:,features].values\n",
        "    y_train = df['TenYearCHD'].values\n",
        "    return X_train, y_train, features\n",
        "\n",
        "# 前処理（平均値による欠損値の補完、正規化）\n",
        "def preprocess(X_train): \n",
        "  # 平均値による単一代入法\n",
        "    simple_imp = SimpleImputer(missing_values=np.nan, \n",
        "  strategy='mean') \n",
        "    simple_imp.fit(X_train ) \n",
        "    X_train = simple_imp.transform(X_train)\n",
        "    ms = MinMaxScaler() \n",
        "    ms.fit(X_train)\n",
        "    X_train = ms.transform(X_train)\n",
        "    return X_train\n",
        "\n",
        "# RFEによる特徴選択と学習\n",
        "def select_by_rfe(n_features,features,\n",
        "    X_train,y_train,X_test,y_test): \n",
        "    svc = SVC(kernel='linear', gamma=1/2 ,  \n",
        "          C=1.0,class_weight='balanced',random_state=0)\n",
        "    rfec = RFE(svc, n_features_to_select= \\\n",
        "              n_features,step=10, verbose=1)\n",
        "    rfec.fit(X_train, y_train) \n",
        "    preds = rfec.predict(X_test) \n",
        "    print(\"RFE + SVC\", rfec.n_features_)\n",
        "    print(clf_report(y_test, preds, digits=3))\n",
        "    feature_ranks = list(zip(rfec.ranking_, features)) \n",
        "    sorted_feature_ranks = sorted(\n",
        "                           feature_ranks, reverse=False)\n",
        "  # 選択された特徴を表示する\n",
        "    for i, fs in enumerate(sorted_feature_ranks[:n_features]):\n",
        "      print('[%d]: %s\\t%.2lf' % (i+1, fs[1], fs[0]))\n",
        "\n",
        "def main(): \n",
        "    # n_features 個に絞り込む\n",
        "    n_features = 8\n",
        "    X_train, y_train, features = prepare()\n",
        "    X_train = preprocess(X_train)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "               X_train, y_train, \n",
        "                random_state=0, train_size=0.8)\n",
        "  \n",
        "    # SVMによる学習・分類（特徴選択なし）\n",
        "    print('- training SVM with default parameters -')\n",
        "    svc = SVC(kernel='linear', gamma=1/2,\n",
        "              C=1.0, class_weight='balanced', random_state=0) \n",
        "    svc.fit(X_train, y_train)\n",
        "    pred = svc.predict(X_test)\n",
        "    print(clf_report(y_test, pred))\n",
        "    # SVM-RFEを用いて特徴選択し、\n",
        "    # n-features個の特徴で分類、学習\n",
        "    print('-*-*-*- Select by SVM-RFE -*-*-*-')\n",
        "    select_by_rfe(n_features, features, X_train, y_train,\n",
        "   X_test, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading heart-disease-prediction-using-logistic-regression.zip to /content\n",
            "\r  0% 0.00/58.4k [00:00<?, ?B/s]\n",
            "\r100% 58.4k/58.4k [00:00<00:00, 21.7MB/s]\n",
            "Archive:  heart-disease-prediction-using-logistic-regression.zip\n",
            "  inflating: framingham.csv          \n",
            "- training SVM with default parameters -\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.66      0.77       710\n",
            "           1       0.28      0.70      0.40       138\n",
            "\n",
            "    accuracy                           0.67       848\n",
            "   macro avg       0.60      0.68      0.59       848\n",
            "weighted avg       0.81      0.67      0.71       848\n",
            "\n",
            "-*-*-*- Select by SVM-RFE -*-*-*-\n",
            "Fitting estimator with 15 features.\n",
            "RFE + SVC 8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.922     0.662     0.770       710\n",
            "           1      0.290     0.710     0.412       138\n",
            "\n",
            "    accuracy                          0.670       848\n",
            "   macro avg      0.606     0.686     0.591       848\n",
            "weighted avg      0.819     0.670     0.712       848\n",
            "\n",
            "[1]: BMI\t1.00\n",
            "[2]: age\t1.00\n",
            "[3]: cigsPerDay\t1.00\n",
            "[4]: diabetes\t1.00\n",
            "[5]: male\t1.00\n",
            "[6]: prevalentHyp\t1.00\n",
            "[7]: sysBP\t1.00\n",
            "[8]: totChol\t1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rb7tY6dsF8P"
      },
      "source": [
        "### プログラム3.18 Borutaを用いた特徴選択"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eya-0td4sLnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139e55c8-632a-42ef-9f1c-dbc8b824a91b"
      },
      "source": [
        "# BorutaPyのインストール\n",
        "!pip install Boruta\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report as clf_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from boruta import BorutaPy\n",
        "\n",
        "# データの準備\n",
        "def prepare():\n",
        "    !kaggle datasets download -d dipam7/student-grade-prediction\n",
        "    !unzip student-grade-prediction.zip\n",
        "    # ポルトガル語学校の学生の成績の予測データセット\n",
        "    df = pd.read_csv('student-mat.csv')\n",
        "    # 欠損値を除去\n",
        "    df = df.dropna()\n",
        "    # 性別を数値に変換\n",
        "    df['sex'] = df['sex'].map({\"F\": 0, \"M\": 1}).astype(int)\n",
        "    # 使用する特徴量\n",
        "    features = ['sex', 'age','Medu', 'Fedu', 'traveltime', \n",
        "                'studytime', 'failures', 'famrel', \n",
        "                 'freetime', 'goout', 'Dalc',\n",
        "                 'Walc', 'health', 'absences']\n",
        "    X = df.loc[:,features].values\n",
        "    #　成績ラベルG2（0 to 20)\n",
        "    y = df['G2'].ravel()\n",
        "    # ビニングにより成績を2クラスに変換\n",
        "    bins = [-1, 10, 20]\n",
        "    labels = ['bad', 'good']\n",
        "    y_cut = pd.cut(y, bins=bins, labels=labels)\n",
        "    print(y_cut)\n",
        "    y = [c for c in y_cut.codes]\n",
        "\n",
        "    return X, y, features, labels\n",
        "\n",
        "# Borutaによる特徴選択\n",
        "def feature_select_by_Boruta(rfc, X, y, features):\n",
        "    # Boruta による特徴選択を定義\n",
        "    feat_selector = BorutaPy(rfc, n_estimators='auto',\n",
        "   verbose=0, random_state=1)\n",
        "    # 関連する特徴の選択\n",
        "    feat_selector.fit(X, y)\n",
        "    # 選択された特徴のチェック\n",
        "    result = feat_selector.support_\n",
        "    print('=====Selected Features=====')\n",
        "    for i,tf in enumerate(result):\n",
        "      if tf == True:\n",
        "        print('%s' % features[i])\n",
        "    \n",
        "    # 特徴量のランキング\n",
        "    ranking = feat_selector.ranking_\n",
        "    rank = {}\n",
        "    for i in range(len(ranking)):\n",
        "      rank[i] = ranking[i]\n",
        "    print('======Feature Ranking======')\n",
        "    for k,v in sorted(rank.items(), key=lambda x:x[1]):\n",
        "      print('[%d]\\t%s' % (v, features[k]) )\n",
        "    # 選択された特徴のみのデータに変換\n",
        "    X_filtered = feat_selector.transform(X) \n",
        "    return X_filtered, feat_selector\n",
        "def main():\n",
        "    X, y, features, target_names = prepare()\n",
        "    X, X_test, y, y_test = train_test_split(X, y, random_state=0, train_size=0.8)\n",
        "    rfc = RandomForestClassifier(n_jobs=-1, \n",
        "         class_weight='balanced', \n",
        "        max_depth=5)\n",
        "    # Boruta により特徴選択\n",
        "    X_filtered, feat_selector = feature_select_by_Boruta(\n",
        "                                    rfc, X, y, features)\n",
        "    # 特徴選択せずにRandomForestで学習・予測\n",
        "    print('Result: all features')\n",
        "    rfc.fit(X,y)\n",
        "    y_pred = rfc.predict(X_test)\n",
        "    print(clf_report(y_test, y_pred, \n",
        "                     target_names= target_names)) \n",
        "    # 特徴選択の結果を用いてRandomForestで学習・予測\n",
        "    print('Result: selected features')\n",
        "    rfc_boruta = RandomForestClassifier(n_jobs=-1, \n",
        "                  class_weight='balanced', max_depth=5)\n",
        "    rfc_boruta.fit(X_filtered, y)\n",
        "    X_test_filtered = feat_selector.transform(X_test)\n",
        "    y_pred_boruta = rfc_boruta.predict(X_test_filtered)\n",
        "    print( clf_report(y_test, y_pred_boruta, \n",
        "                      target_names =target_names))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Boruta\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/11/583f4eac99d802c79af9217e1eff56027742a69e6c866b295cce6a5a8fc2/Boruta-0.3-py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▉                          | 10kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 20kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 40kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from Boruta) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from Boruta) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from Boruta) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17.1->Boruta) (0.17.0)\n",
            "Installing collected packages: Boruta\n",
            "Successfully installed Boruta-0.3\n",
            "Downloading student-grade-prediction.zip to /content\n",
            "  0% 0.00/7.16k [00:00<?, ?B/s]\n",
            "100% 7.16k/7.16k [00:00<00:00, 12.2MB/s]\n",
            "Archive:  student-grade-prediction.zip\n",
            "  inflating: student-mat.csv         \n",
            "['bad', 'bad', 'bad', 'good', 'bad', ..., 'bad', 'good', 'bad', 'good', 'bad']\n",
            "Length: 395\n",
            "Categories (2, object): ['bad' < 'good']\n",
            "=====Selected Features=====\n",
            "failures\n",
            "absences\n",
            "======Feature Ranking======\n",
            "[1]\tfailures\n",
            "[1]\tabsences\n",
            "[2]\tFedu\n",
            "[3]\tMedu\n",
            "[3]\tgoout\n",
            "[5]\tfreetime\n",
            "[6]\tstudytime\n",
            "[6]\tfamrel\n",
            "[8]\thealth\n",
            "[9]\tage\n",
            "[10]\ttraveltime\n",
            "[11]\tWalc\n",
            "[12]\tDalc\n",
            "[13]\tsex\n",
            "Result: all features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         bad       0.59      0.54      0.56        37\n",
            "        good       0.62      0.67      0.64        42\n",
            "\n",
            "    accuracy                           0.61        79\n",
            "   macro avg       0.61      0.60      0.60        79\n",
            "weighted avg       0.61      0.61      0.61        79\n",
            "\n",
            "Result: selected features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         bad       0.64      0.49      0.55        37\n",
            "        good       0.63      0.76      0.69        42\n",
            "\n",
            "    accuracy                           0.63        79\n",
            "   macro avg       0.64      0.62      0.62        79\n",
            "weighted avg       0.63      0.63      0.63        79\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBI90T4GtDUn"
      },
      "source": [
        "### プログラム3.19 リッジ回帰、Lasso回帰、Elastic Net による特徴選択および予測モデルの学習と評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okfO9sPvtLwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc9af23-0750-412c-d22e-1a153f781aee"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 回帰モデルを基に特徴選択\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "# リッジ回帰, Lasso回帰, Elastic Net\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "\n",
        "\n",
        "# カテゴリ変数を数字に変換\n",
        "def replace_to_digit(dff):\n",
        "    t = list(set([v for v in dff.values])) \n",
        "    t.sort()\n",
        "    vals = [t.index(v) for v in dff.values]\n",
        "    return vals\n",
        "\n",
        "# データの準備\n",
        "def prepare():\n",
        "    !kaggle datasets download -d \\\n",
        "    mylesoneill/world-university-rankings\n",
        "    !unzip world-university-rankings.zip\n",
        "    # 世界大学ランキングのデータを使用\n",
        "    # 分類に使用する特徴量\n",
        "    features = ['country', 'national_rank', \n",
        "                'quality_of_education',\n",
        "                'alumni_employment', 'quality_of_faculty',\n",
        "                'publications', 'influence',\n",
        "                'citations', 'broad_impact',\n",
        "                'patents', 'score']\n",
        "\n",
        "    df_train = pd.read_csv('cwurData.csv')\n",
        "    df_train['country'] = \\\n",
        "    replace_to_digit(df_train['country'])\n",
        "    X_train = df_train.loc[:,features].values\n",
        "    y_train = df_train.loc[:,['world_rank']].values.ravel()\n",
        "    # ビニングによりランキングを4分割\n",
        "    bins = [0, 250, 500, 750, 1000]\n",
        "    labels = [0, 1, 2, 3]\n",
        "    classNames = ['(0,250]', '(250,500]', \n",
        "  '(500,750]', '(750,1000]']\n",
        "    y_cut = pd.cut(\n",
        "  df_train.loc[:,['world_rank']].values.ravel(),   bins=bins, labels=labels)\n",
        "    y_train = [c for c in y_cut.codes]\n",
        "    return X_train, y_train, features, classNames\n",
        "  \n",
        "# 前処理（平均値による欠損値の補完、正規化）\n",
        "def preprocess(X_train): \n",
        "    # 平均値による単一代入法\n",
        "    simple_imp = SimpleImputer(missing_values=np.nan, \n",
        "                               strategy='mean') \n",
        "    simple_imp.fit(X_train) \n",
        "    X_train = simple_imp.transform(X_train) \n",
        "    ms = MinMaxScaler()\n",
        "    # 正規化\n",
        "    ms.fit(X_train)\n",
        "    X_train = ms.transform(X_train)\n",
        "    return X_train\n",
        "\n",
        "def main(): \n",
        "    X_train, y_train, features, classNames = prepare()\n",
        "    print('Original Features ', features)\n",
        "    X_train = preprocess(X_train)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                       X_train, y_train, \n",
        "                                       random_state=1, \n",
        "                                       train_size=0.8)\n",
        "    # 特徴選択無しを含め4パターンの方法で評価\n",
        "    selectors = [None, Ridge(alpha=-0.6), \n",
        "                 Lasso(alpha=0.02),\n",
        "                ElasticNet(alpha=0.0001, l1_ratio=0.7)] \n",
        "    for sel in selectors:\n",
        "        if sel == None:\n",
        "            print('Logistic Regression \\\n",
        "                 without Feature selection')\n",
        "            X_train_sel = X_train\n",
        "            X_test_sel = X_test\n",
        "        else:\n",
        "            s_f = SelectFromModel(sel) \n",
        "            s_f.fit(X_train, y_train)\n",
        "            print('-- Selected Features by {} --'.format(\n",
        "                    sel.__class__.__name__))\n",
        "            for i, f in enumerate(s_f.get_support()):\n",
        "                if f == 1:\n",
        "                    print('%d: %s' % (f, features[i]))\n",
        "            X_train_sel = s_f.transform(X_train)\n",
        "            X_test_sel = s_f.transform(X_test)\n",
        "            print('Logistic Regression by {} Feature \\\n",
        "                         selection'.format(sel.__class__.__name__))\n",
        "        lr = LogisticRegression(max_iter=150)\n",
        "        lr.fit(X_train_sel, y_train)\n",
        "        print('\\tTest set Accuracy: %.3lf\\n' % lr.score( \\\n",
        "        X_test_sel, y_test))\n",
        "        y_pred = lr.predict(X_test_sel)\n",
        "        print(classification_report(y_test, y_pred,\n",
        "   target_names=classNames))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading world-university-rankings.zip to /content\n",
            "\r  0% 0.00/1.41M [00:00<?, ?B/s]\n",
            "\r100% 1.41M/1.41M [00:00<00:00, 91.4MB/s]\n",
            "Archive:  world-university-rankings.zip\n",
            "  inflating: cwurData.csv            \n",
            "  inflating: education_expenditure_supplementary_data.csv  \n",
            "  inflating: educational_attainment_supplementary_data.csv  \n",
            "  inflating: school_and_country_table.csv  \n",
            "  inflating: shanghaiData.csv        \n",
            "  inflating: timesData.csv           \n",
            "Original Features  ['country', 'national_rank', 'quality_of_education', 'alumni_employment', 'quality_of_faculty', 'publications', 'influence', 'citations', 'broad_impact', 'patents', 'score']\n",
            "Logistic Regression                  without Feature selection\n",
            "\tTest set Accuracy: 0.850\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     (0,250]       0.92      0.96      0.94       140\n",
            "   (250,500]       0.80      0.79      0.80       106\n",
            "   (500,750]       0.77      0.73      0.75        94\n",
            "  (750,1000]       0.88      0.87      0.87       100\n",
            "\n",
            "    accuracy                           0.85       440\n",
            "   macro avg       0.84      0.84      0.84       440\n",
            "weighted avg       0.85      0.85      0.85       440\n",
            "\n",
            "-- Selected Features by Ridge --\n",
            "1: alumni_employment\n",
            "1: publications\n",
            "1: broad_impact\n",
            "1: score\n",
            "Logistic Regression by Ridge Feature                          selection\n",
            "\tTest set Accuracy: 0.852\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     (0,250]       0.93      0.91      0.92       140\n",
            "   (250,500]       0.76      0.76      0.76       106\n",
            "   (500,750]       0.77      0.79      0.78        94\n",
            "  (750,1000]       0.91      0.92      0.92       100\n",
            "\n",
            "    accuracy                           0.85       440\n",
            "   macro avg       0.85      0.85      0.85       440\n",
            "weighted avg       0.85      0.85      0.85       440\n",
            "\n",
            "-- Selected Features by Lasso --\n",
            "1: quality_of_education\n",
            "1: alumni_employment\n",
            "1: publications\n",
            "1: influence\n",
            "1: citations\n",
            "1: broad_impact\n",
            "1: patents\n",
            "Logistic Regression by Lasso Feature                          selection\n",
            "\tTest set Accuracy: 0.850\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     (0,250]       0.92      0.96      0.94       140\n",
            "   (250,500]       0.81      0.77      0.79       106\n",
            "   (500,750]       0.76      0.76      0.76        94\n",
            "  (750,1000]       0.88      0.87      0.87       100\n",
            "\n",
            "    accuracy                           0.85       440\n",
            "   macro avg       0.84      0.84      0.84       440\n",
            "weighted avg       0.85      0.85      0.85       440\n",
            "\n",
            "-- Selected Features by ElasticNet --\n",
            "1: alumni_employment\n",
            "1: publications\n",
            "1: broad_impact\n",
            "1: score\n",
            "Logistic Regression by ElasticNet Feature                          selection\n",
            "\tTest set Accuracy: 0.852\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     (0,250]       0.93      0.91      0.92       140\n",
            "   (250,500]       0.76      0.76      0.76       106\n",
            "   (500,750]       0.77      0.79      0.78        94\n",
            "  (750,1000]       0.91      0.92      0.92       100\n",
            "\n",
            "    accuracy                           0.85       440\n",
            "   macro avg       0.85      0.85      0.85       440\n",
            "weighted avg       0.85      0.85      0.85       440\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHvT9LZAteg7"
      },
      "source": [
        "### プログラム3.20 RidgeCV, LassoCV, ElasticNetCV を用いて、パラメータ選択し、予測モデルの学習と評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY0i6H7ouO2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56ba595-00fa-4f3e-b63f-8191598b9033"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 回帰モデルを基に特徴選択\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "# リッジ回帰, Lasso回帰, RidgeCV, LassoCVをインポート\n",
        "from sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV\n",
        "# ElasticNet, ElasticNetCV をインポート\n",
        "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
        "\n",
        "# カテゴリ変数を数字に変換\n",
        "def replace_to_digit(dff):\n",
        "    t = list(set([v for v in dff.values])) \n",
        "    t.sort()\n",
        "    vals = [t.index(v) for v in dff.values]\n",
        "    return vals\n",
        "\n",
        "# データの準備\n",
        "def prepare():\n",
        "    #!kaggle datasets download -d \\\n",
        "    #mylesoneill/world-university-rankings\n",
        "    #!unzip world-university-rankings.zip\n",
        "    # 世界大学ランキングのデータを使用\n",
        "    # 分類に使用する特徴量\n",
        "    features = ['country', 'national_rank', \n",
        "                'quality_of_education',\n",
        "                'alumni_employment', 'quality_of_faculty',\n",
        "                'publications', 'influence',\n",
        "                'citations', 'broad_impact',\n",
        "                'patents', 'score']\n",
        "\n",
        "    df_train = pd.read_csv('cwurData.csv')\n",
        "    df_train['country'] = \\\n",
        "    replace_to_digit(df_train['country'])\n",
        "    X_train = df_train.loc[:,features].values\n",
        "    y_train = df_train.loc[:,['world_rank']].values.ravel()\n",
        "    # ビニングによりランキングを4分割\n",
        "    bins = [0, 250, 500, 750, 1000]\n",
        "    labels = [0, 1, 2, 3]\n",
        "    classNames = ['(0,250]', '(250,500]', \n",
        "  '(500,750]', '(750,1000]']\n",
        "    y_cut = pd.cut(\n",
        "  df_train.loc[:,['world_rank']].values.ravel(),   bins=bins, labels=labels)\n",
        "    y_train = [c for c in y_cut.codes]\n",
        "    return X_train, y_train, features, classNames\n",
        "  \n",
        "# 前処理（平均値による欠損値の補完、正規化）\n",
        "def preprocess(X_train): \n",
        "    # 平均値による単一代入法\n",
        "    simple_imp = SimpleImputer(missing_values=np.nan, \n",
        "                               strategy='mean') \n",
        "    simple_imp.fit(X_train) \n",
        "    X_train = simple_imp.transform(X_train) \n",
        "    ms = MinMaxScaler()\n",
        "    # 正規化\n",
        "    ms.fit(X_train)\n",
        "    X_train = ms.transform(X_train)\n",
        "    return X_train\n",
        "\n",
        "# main\n",
        "def main(): \n",
        "    X_train, y_train, features, classNames = prepare()\n",
        "    print('Original Features ', features)\n",
        "    X_train = preprocess(X_train)\n",
        "    X_train, X_test, y_train, y_test = train_test_split( \n",
        "                                       X_train, y_train, \n",
        "                                         random_state=0, \n",
        "                                         train_size=0.9)\n",
        "    alphas = (0.01, 0.5, 1.0) \n",
        "    cvs = [RidgeCV(alphas=alphas), \n",
        "           LassoCV(alphas=alphas), \n",
        "           ElasticNetCV(alphas=alphas)]\n",
        "    selectors = [Ridge(), Lasso(), ElasticNet()]\n",
        "    for cv, sel in zip(cvs, selectors): \n",
        "        # リッジ回帰、Lasso回帰、ElasticNetを\n",
        "        # の正則化パラメータの選択および、選択された\n",
        "        # パラメータを用いた特徴選択、\n",
        "        # ロジスティック回帰による学習・予測\n",
        "        print('Logistic Regression with {} \\\n",
        "               Feature Selection'.format(cv.__class__.__name__))\n",
        "        cv.fit(X_train, y_train)\n",
        "        print(cv.alpha_)\n",
        "        sel.alpha = cv.alpha_\n",
        "        s_f = SelectFromModel(sel) \n",
        "        s_f.fit(X_train, y_train)\n",
        "        print('- Selected Features \\\n",
        "                by {} -'.format(sel.__class__.__name__))\n",
        "        for i, f in enumerate(s_f.get_support()):\n",
        "            if f == 1:\n",
        "                print('%d: %s' % (f, features[i]))\n",
        "\n",
        "        X_train_sel = s_f.transform(X_train)\n",
        "        X_test_sel = s_f.transform(X_test)\n",
        "        lr = LogisticRegression()\n",
        "        lr.fit(X_train_sel, y_train)\n",
        "        y_pred = lr.predict(X_test_sel)\n",
        "        print('\\tTest set Accuracy: %.3lf\\n' % lr.score(\n",
        "                X_test_sel, y_test) )\n",
        "        print(classification_report(y_test, y_pred,\n",
        "                target_names=classNames ))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Features  ['country', 'national_rank', 'quality_of_education', 'alumni_employment', 'quality_of_faculty', 'publications', 'influence', 'citations', 'broad_impact', 'patents', 'score']\n",
            "Logistic Regression with RidgeCV                Feature Selection\n",
            "0.5\n",
            "- Selected Features                 by Ridge -\n",
            "1: alumni_employment\n",
            "1: publications\n",
            "1: broad_impact\n",
            "1: score\n",
            "\tTest set Accuracy: 0.855\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     (0,250]       0.91      0.91      0.91        68\n",
            "   (250,500]       0.80      0.75      0.78        60\n",
            "   (500,750]       0.74      0.80      0.77        44\n",
            "  (750,1000]       0.94      0.96      0.95        48\n",
            "\n",
            "    accuracy                           0.85       220\n",
            "   macro avg       0.85      0.85      0.85       220\n",
            "weighted avg       0.85      0.85      0.85       220\n",
            "\n",
            "Logistic Regression with LassoCV                Feature Selection\n",
            "0.01\n",
            "- Selected Features                 by Lasso -\n",
            "1: quality_of_education\n",
            "1: alumni_employment\n",
            "1: publications\n",
            "1: influence\n",
            "1: citations\n",
            "1: broad_impact\n",
            "1: patents\n",
            "\tTest set Accuracy: 0.832\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     (0,250]       0.92      0.90      0.91        68\n",
            "   (250,500]       0.79      0.73      0.76        60\n",
            "   (500,750]       0.68      0.77      0.72        44\n",
            "  (750,1000]       0.92      0.92      0.92        48\n",
            "\n",
            "    accuracy                           0.83       220\n",
            "   macro avg       0.83      0.83      0.83       220\n",
            "weighted avg       0.84      0.83      0.83       220\n",
            "\n",
            "Logistic Regression with ElasticNetCV                Feature Selection\n",
            "0.01\n",
            "- Selected Features                 by ElasticNet -\n",
            "1: alumni_employment\n",
            "1: publications\n",
            "1: influence\n",
            "1: broad_impact\n",
            "\tTest set Accuracy: 0.864\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     (0,250]       0.94      0.93      0.93        68\n",
            "   (250,500]       0.82      0.78      0.80        60\n",
            "   (500,750]       0.74      0.77      0.76        44\n",
            "  (750,1000]       0.92      0.96      0.94        48\n",
            "\n",
            "    accuracy                           0.86       220\n",
            "   macro avg       0.86      0.86      0.86       220\n",
            "weighted avg       0.86      0.86      0.86       220\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}